import requests
import time
from bs4 import BeautifulSoup
import random

head = {'Accept':'*/*',
'Accept-Language':'zh-CN,zh;q=0.8',
'Connection':'keep-alive',
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
'Referer':'http//www.amazon.cn/'
}
cookie = {'x-wl-uid':'1fCUQJ7PN7sLG8QBUblS+xYHbzoLUVlzgqsr+UOnT4nfhj3ZrH2zPiI5KWJgCgSQCcRdqfdTV4yjsDYW91JHUQmfTLWISVN750ysi0lEA3+BG6gJ7rlrRI1qQeFL1lrgSeLz66GMfY0c=',
's_vnum':'1943941727867%26vn%3D2',
's_nr':'1512024557655-Repeat',
's_dslv':'1512024557657',
'session-token':'"Nj7b6nUcacJIfx+yhjvsO23hoEyE0Dii+D52Bj8EB+BJnhAGqaJYjsilsCo96hYfElPWf3tBce5PvGlf1FD8Fc07rIv1BZmbvBNPc68rAROngKI8JaR+a4Rrgzw/OR1bU6RjYI3xavoUCT8JOJ01tMu9utd1KCVz25QlRGkTDEyBKmN57zeELr8wuj6QJO4wkMtU2FyjkwyCfm3Mp4o407cjVjB7R0C6TfztCcA4bqhTt+YamgkgiQ=="',
'ubid-acbcn':'457-7671090-8973506',
'session-id-time':'2082729601l',
'session-id':'462-0029283-0476566',
'__guid':'40700661.731519576838389100.1513177678139.59',
'monitor_count':'2'}

for i in range(1,1178):
#i = 4
    url = 'http://www.amazon.cn/Kindle-Paperwhite%E7%94%B5%E5%AD%90%E4%B9%A6%E9%98%85%E8%AF%BB%E5%99%A8-300-ppi%E8%B6%85%E6%B8%85%E7%94%B5%E5%AD%90%E5%A2%A8%E6%B0%B4%E8%A7%A6%E6%8E%A7%E5%B1%8F-%E5%86%85%E7%BD%AE%E9%98%85%E8%AF%BB%E7%81%AF-%E8%B6%85%E9%95%BF%E7%BB%AD%E8%88%AA/product-reviews/B00QJDOLIO/ref=cm_cr_getr_d_paging_btm_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}'.format(i,i)
    r = requests.get(url, headers = head, cookies = cookie)
    html = r.content
    soup = BeautifulSoup(html, "lxml")
    div = soup.find_all('span',attrs={'data-hook':'review-body'})
    print('*'*20+'现在正在下载第'+str(i)+'页评论'+'*'*20)
    print(r)
    for x in div:
        file = open(r'C:\Users\sco\Desktop\AmazonData.txt','a',encoding='utf-8')
        file.write(str(div))
        file.close
    time.sleep(0.1)
    #time.sleep(random.randint(1,3))后来发现太慢了，就用了上面的延时0.1s，快一点
